{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare environment"
      ],
      "metadata": {
        "id": "MdW6ZV82wKFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown -q\n",
        "!pip install annoy -q\n",
        "# Generate trie for bigrams\n",
        "!pip install pygtrie\n",
        "!pip install jsonlines - q"
      ],
      "metadata": {
        "id": "YPVenqSAwNq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "import pygtrie\n",
        "import random\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "import gdown\n",
        "import urllib.request\n",
        "import gzip\n",
        "import shutil\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YiitOB4wPC9",
        "outputId": "8a3febe8-fbe8-47d6-8a18-57210927cf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load bigrams and fivegrams data\n",
        "ids = [\"1LauUKpATAXc6rvY5xKhS7rFWoJRxYt93\", \"1N7uOUeP8rPnLfXSoJV4L4PN-R1uh-bd9\"]\n",
        "filenames = [\"bigrams.txt\", \"fivegrams.txt\"]\n",
        "url = 'https://drive.google.com/uc?id='\n",
        "\n",
        "for i in range(len(ids)):\n",
        "    gdown.download(url+ids[i], filenames[i], quiet=True)"
      ],
      "metadata": {
        "id": "4Y7Vfob4wUDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "jil2aJdSwtlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sent):\n",
        "    # Replace punctuation with space\n",
        "    sent = sent.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation), ''))\n",
        "    # Get word tokens\n",
        "    sent = word_tokenize(sent)\n",
        "    words = []\n",
        "    for w in sent:\n",
        "        # Only-digits words will not be considered as a word\n",
        "        # While te1xt may be a misspell\n",
        "        if not w.isdigit():\n",
        "            words.append(w)\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "UVHSCvDY8hI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary:\n",
        "    \"\"\"\n",
        "    I created a class called Dictionary to handle data, such as bigrams and fivegrams data, list of unique words\n",
        "    (used as dictionary words) and characters of a language. Note that I considered only English words, so characters\n",
        "    of any other language will be considered as unknown character. Nevertheless, this implementation may be improved\n",
        "    by adding other languages data (bigrams, unique words and characters).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bigram_path=\"bigrams.txt\", fivegram_path=\"fivegrams.txt\"):\n",
        "        \"\"\"\n",
        "        Prepare data and tries\n",
        "        :param bigram_path: path to bigram dataset\n",
        "        :param fivegram_path: path to fivegram dataset\n",
        "        \"\"\"\n",
        "        bigram_data = self.load_bigrams(bigram_path)\n",
        "        # Get unique words in bigrams dataset\n",
        "        self.unique_words = np.unique([bigram_data[\"0\"].astype(str).apply(str.lower).values,\n",
        "                                       bigram_data[\"1\"].astype(str).apply(str.lower).values])\n",
        "        self.bigram_trie = self.build_bigram_trie(bigram_data)\n",
        "\n",
        "        self.chars = \"A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z\".lower().split(\n",
        "            \", \") + [\"<unk>\"]\n",
        "        self.word_trie = self.build_word_trie(trie_depth=21, random_seed=33, filename='word_char_count.ann')\n",
        "\n",
        "    def load_bigrams(self, path=\"bigrams.txt\"):\n",
        "        \"\"\"\n",
        "        Load bigrams.\n",
        "        As a bigram dataset [2], I used the data provided with this assignment.\n",
        "        This data contains most frequent n-grams from the one billion word\n",
        "        Corpus of Contemporary American English (COCA).\n",
        "\n",
        "        :param path: path to bigram dataset\n",
        "        :return: pd dataframe [freq, 0, 1, prob]\n",
        "        \"\"\"\n",
        "        bigram_data = pd.read_csv(path, sep=\"\\t\", encoding=\"ISO-8859-1\", header=None, names=[\"freq\", \"0\", \"1\"])\n",
        "        bigram_data[[\"0\", \"1\"]] = bigram_data[[\"0\", \"1\"]].astype(str).apply(lambda x: [y.lower() for y in x])\n",
        "        # Move from frequency to probability by dividing on total amount of occurences\n",
        "        bigram_total = bigram_data.groupby(\"0\")[\"freq\"].sum()\n",
        "        len_unique_words = len(self.dictionary.unique_words)\n",
        "        bigram_data['prob'] = bigram_data.apply(lambda row: row['freq'] / bigram_total[row['0']], axis=1)\n",
        "        return bigram_data\n",
        "\n",
        "    def load_fivegrams(self, path=\"fivegrams.txt\"):\n",
        "        \"\"\"\n",
        "        Load fivegrams.\n",
        "        As a fivegram dataset [2], I used the data provided with this assignment.\n",
        "        This data contains most frequent n-grams from the one billion word\n",
        "        Corpus of Contemporary American English (COCA).\n",
        "        :param path: path to fivegram dataset\n",
        "        :return: pd dataframe [freq, 0, 1,2,3,4, prob]\n",
        "        \"\"\"\n",
        "        fivegram_data = pd.read_csv(path, sep=\"\\t\", encoding=\"ISO-8859-1\", header=None,\n",
        "                                    names=[\"freq\", \"0\", \"1\", \"2\", \"3\", \"4\"])\n",
        "        fivegram_data[[\"0\", \"1\", \"2\", \"3\", \"4\"]] = fivegram_data[[\"0\", \"1\", \"2\", \"3\", \"4\"]].astype(str).apply(\n",
        "            lambda x: [y.lower() for y in x])\n",
        "\n",
        "        # Move from frequency to probability by dividing on total amount of occurences\n",
        "        # From five to fourgrams, because too much probabilities = 1s\n",
        "        fivegram_total = fivegram_data.groupby([\"0\", \"1\", \"2\"])[\"freq\"].sum()\n",
        "        fivegram_data['prob'] = fivegram_data.apply(\n",
        "            lambda row: row['freq'] / fivegram_total[row['0'], row['1'], row['2']], axis=1)\n",
        "\n",
        "        fivegram_data.groupby(['prob'])['freq'].count().sort_values()\n",
        "        return fivegram_data\n",
        "\n",
        "    def build_bigram_trie(self, bigram_data):\n",
        "        \"\"\"\n",
        "        Build a trie for faster bigram search.\n",
        "        The problem I faced during running first version of my code is a slow processing of words.\n",
        "        I remembered a trie structures that may improve search on text strings.\n",
        "        One of that, CharTrie from pygtrie [4], I used to speed up bigram search.\n",
        "        The key is a string of format (first_element + \" \" + second_element) and value is a probability\n",
        "        of such a bigram. Note that this probability was obtained by dividing number of bigram occurence on the\n",
        "        number of first word count in the dataset. According to lectures [1], the smoothing is a good improvement,\n",
        "        so I used one with 1/num_unique_words as a base.\n",
        "\n",
        "        :param bigram_data: bigrams dataset\n",
        "        :return: pygtrie.CharTrie to find bigrams\n",
        "        \"\"\"\n",
        "        bigram_trie = pygtrie.CharTrie()\n",
        "\n",
        "        for ind, row in bigram_data.iterrows():\n",
        "            sent = row['0'] + \" \" + row['1']\n",
        "            # Update the trie key values\n",
        "            if bigram_trie.has_key(sent):\n",
        "                print(sent)\n",
        "            else:\n",
        "                bigram_trie[sent] = row['prob']\n",
        "        return bigram_trie\n",
        "\n",
        "    def word_vectorize(self, word):\n",
        "        \"\"\"\n",
        "        Made a vector as a number of occurences of each english letter in the word and, additionally,\n",
        "        unknown character (all symbols that are not English characters).\n",
        "        :param word: word to be vectorized\n",
        "        :return: vector shape (27,)\n",
        "        \"\"\"\n",
        "        vec = [word.lower().count(c) for c in self.chars[:-1]]\n",
        "        vec.append(sum([not c.isalpha() for c in word.lower()]))\n",
        "        return vec\n",
        "\n",
        "    def build_word_trie(self, trie_depth=21, random_seed=33, filename='word_char_count.ann'):\n",
        "        \"\"\"\n",
        "        One more slow process was finding a similar word to the misspelled. The Edit distance algorithm is not so fast,\n",
        "        and the number of unique words is big, so more optimized search was needed. As a solution, I made a vector\n",
        "        from each word and put it inside the annoy algorithm [5].\n",
        "        The manhattan distance was used with annoy algorithm, as an approximation to edit distance.\n",
        "        :param trie_depth: depth of the trie\n",
        "        :param random_seed: random seed\n",
        "        :param filename: filename where to store the result\n",
        "        :return: Annoy index\n",
        "        \"\"\"\n",
        "        trie = AnnoyIndex(len(self.chars), 'manhattan')\n",
        "        trie.set_seed(random_seed)\n",
        "\n",
        "        # Put data in the index\n",
        "        for ind, word in enumerate(self.unique_words):\n",
        "            trie.add_item(ind, self.word_vectorize(word))\n",
        "        # Build trie\n",
        "        trie.build(trie_depth)\n",
        "        # Store result\n",
        "        if filename is not None:\n",
        "            trie.save(filename)\n",
        "        return trie\n",
        "\n",
        "    def load_word_trie(self, filename='word_char_count.ann'):\n",
        "        \"\"\"\n",
        "        Load word trie explained above\n",
        "        :param filename: from where to load the index\n",
        "        :return: annoy trie (index)\n",
        "        \"\"\"\n",
        "        trie = AnnoyIndex(len(self.chars), 'manhattan')\n",
        "        trie.load(filename)\n",
        "        return trie\n",
        "\n",
        "    def get_nearest_words(self, word, k=150):\n",
        "        \"\"\"\n",
        "        Find the most similar words from the dictionary using annoy trie\n",
        "        :param word: intial word\n",
        "        :param k: max number of  similar words to output\n",
        "        :return: list of similar words\n",
        "        \"\"\"\n",
        "        return [self.unique_words[i] for i in self.word_trie.get_nns_by_vector(self.word_vectorize(word), k)]\n"
      ],
      "metadata": {
        "id": "FiqRvrafwvjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EditDistance:\n",
        "    \"\"\"\n",
        "    Damreu-Levenstein variant with QWERTY-based weights.\n",
        "    Based on existing research [3, 8], it is reasonable to consider edit distances smaller than or equal to 2.\n",
        "    So I considered one more improvement: if difference in lenghts of misspelled word and a candidate is greater than 2,\n",
        "    this candidate do not analyzed further. The ranked candidates goes through ngrams' probability sort and top one is\n",
        "    returned as the correction.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.keyboard = ['QWERTYUIOP', 'ASDFGHJKL', 'ZXCVBNM']\n",
        "        for i in range(len(self.keyboard)):\n",
        "            self.keyboard[i] = list(self.keyboard[i].lower())\n",
        "\n",
        "    # Calculate edit distance\n",
        "\n",
        "    def _levenshtein_distance_matrix(self, string1, string2, is_damerau=False):\n",
        "        \"\"\"\n",
        "        Calculate Damreu-Levenstein distance with QWERTY-based weights\n",
        "        :param string1: first word\n",
        "        :param string2: second word\n",
        "        :param is_damerau: replace = 1 or 2\n",
        "        :return: distance matrix between words\n",
        "        \"\"\"\n",
        "        n1 = len(string1)\n",
        "        n2 = len(string2)\n",
        "        d = np.zeros((n1 + 1, n2 + 1), dtype=float)\n",
        "        for i in range(n1 + 1):\n",
        "            d[i, 0] = i\n",
        "        for j in range(n2 + 1):\n",
        "            d[0, j] = j\n",
        "        for i in range(n1):\n",
        "            for j in range(n2):\n",
        "                if string1[i] == string2[j]:\n",
        "                    cost = 0\n",
        "                else:\n",
        "                    cost = self.replace_weight(string1[i], string2[j])\n",
        "                    # cost = 0.5\n",
        "                d[i + 1, j + 1] = min(d[i, j + 1] + 1,  # insert\n",
        "                                      d[i + 1, j] + 1,  # delete\n",
        "                                      d[i, j] + cost)  # replace\n",
        "                if is_damerau:\n",
        "                    if i > 0 and j > 0 and string1[i] == string2[j - 1] and string1[i - 1] == string2[j]:\n",
        "                        d[i + 1, j + 1] = min(d[i + 1, j + 1], d[i - 1, j - 1] + cost)  # transpose\n",
        "        return d\n",
        "\n",
        "    def replace_weight(self, let1, let2) -> float:\n",
        "        \"\"\"\n",
        "        The weight for swap 2 letters is 1, while all letter pairs which share vertical border get 0.5 multiplier for\n",
        "        replace and all letter pairs which share at least some horizontal border get 0.7 multiplier for replace.\n",
        "\n",
        "        :param let1: first letter\n",
        "        :param let2: second\n",
        "        :return: distance\n",
        "        \"\"\"\n",
        "        let1 = let1.lower()\n",
        "        let2 = let2.lower()\n",
        "\n",
        "        col1, row1 = 0, 0\n",
        "        col2, row2 = 0, 0\n",
        "\n",
        "        for ind in range(len(self.keyboard)):\n",
        "            line = self.keyboard[ind]\n",
        "            if let1 in line and let2 in line:\n",
        "                if -1 <= line.index(let1) - line.index(let2) <= 1:\n",
        "                    return 0.5\n",
        "                else:\n",
        "                    return 1\n",
        "            elif let1 in line:\n",
        "                col1 = ind\n",
        "                row1 = line.index(let1) + ind / 2\n",
        "            elif let2 in line:\n",
        "                col2 = ind\n",
        "                row2 = line.index(let2) + ind / 2\n",
        "        if not -1 <= col1 - col2 <= 1:\n",
        "            return 1\n",
        "        if -1 <= row1 - row2 <= 1:\n",
        "            return 0.7\n",
        "\n",
        "        return 1\n",
        "\n",
        "    def qwerty_edit_dist(self, s1, s2) -> float:\n",
        "        # compute the Damerau-Levenshtein distance\n",
        "        # between two given strings (s1 and s2)\n",
        "        n1 = len(s1)\n",
        "        n2 = len(s2)\n",
        "        return self._levenshtein_distance_matrix(s1, s2, True)[n1, n2]\n"
      ],
      "metadata": {
        "id": "gsETHSFf8SfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextSpellChecker:\n",
        "    \"\"\"\n",
        "    The main class (and algorithm workflow) is ContextSpellChecker. It takes a sequence of words and finds those who\n",
        "    contains misspell (non-word errors based on list of unique words). For each such word it finds some candidates with\n",
        "    word_trie, ranks them with edit distance and takes most probable one based on the context near it.\n",
        "    \"\"\"\n",
        "    def __init__(self, dictionary, edit_dist, sensitivity=0.5):\n",
        "        self.dictionary = dictionary\n",
        "        self.edit_dist = edit_dist\n",
        "        self.sensitivity = sensitivity\n",
        "\n",
        "    def get_nearest_candidates(self, word, top_k=10):\n",
        "        \"\"\"\n",
        "        Find the most similar words from the dictionary and sort by their edit distance\n",
        "        :param word: initial word\n",
        "        :param top_k: max num output candidates\n",
        "        :return: list of candidates\n",
        "        \"\"\"\n",
        "        global chars\n",
        "        cand_dist = {}\n",
        "        # Apply annoy as approximation\n",
        "        for candidate in self.dictionary.get_nearest_words(word, k=10 * top_k):\n",
        "            if -2 <= len(word) - len(candidate) <= 2:\n",
        "                dist = self.edit_dist.qwerty_edit_dist(word, candidate)\n",
        "                if dist <= 2:\n",
        "                    cand_dist[candidate] = dist\n",
        "        if len(cand_dist) == 0:\n",
        "            return []\n",
        "        # Rank by edit distance\n",
        "        mini = min(cand_dist.values())\n",
        "        res = {}\n",
        "        if mini < 2:\n",
        "            for k in cand_dist.keys():\n",
        "                if cand_dist[k] <= mini + self.sensitivity:\n",
        "                    res[k] = cand_dist[k]\n",
        "        res = sorted(res.items(), key=lambda x: x[1])\n",
        "\n",
        "        if len(res) > top_k:\n",
        "            res = res[:top_k]\n",
        "        return res\n",
        "\n",
        "    # Ngams\n",
        "    def calculate_prob(self, ind_w, candidate, sent, ngram):\n",
        "        \"\"\"\n",
        "        Calculate conditional probability\n",
        "        :param ind_w: word index\n",
        "        :param candidate: candidate to replace the word\n",
        "        :param sent: initial sentence\n",
        "        :param ngram: n from ngram used\n",
        "        :return: conditional probability\n",
        "        \"\"\"\n",
        "        # Make initial probability some number (equal to all candidate)\n",
        "        # to prevent underflow\n",
        "        prob = 10 ** 5\n",
        "        for i in range(max(0, ind_w - ngram + 1), min(len(sent) - ngram + 1, ind_w + ngram)):\n",
        "            # New word sequence\n",
        "            seq = sent[i:i + ngram]\n",
        "            seq[ind_w - i] = candidate[0]\n",
        "            seq = \" \".join(seq)\n",
        "            p = 0\n",
        "            # Get probabiluty\n",
        "            if self.dictionary.bigram_trie.has_key(seq):\n",
        "                p = self.dictionary.bigram_trie[seq]\n",
        "            p += 1 / len(self.dictionary.unique_words)\n",
        "            prob *= p\n",
        "        return prob\n",
        "\n",
        "    def spell_check(self, old_sent):\n",
        "        \"\"\"\n",
        "        Correct sequence of words\n",
        "        Rank by edit, Look at context\n",
        "        :param old_sent: list of words\n",
        "        :return: new list of corrected words\n",
        "        \"\"\"\n",
        "        sent = old_sent.copy()\n",
        "        was_changed = False\n",
        "        for ind_w in range(len(sent)):\n",
        "            word = sent[ind_w]\n",
        "            # Check non-word error\n",
        "            if word not in self.dictionary.unique_words:\n",
        "                candidates = self.get_nearest_candidates(word)\n",
        "\n",
        "                # Get total frequency of words\n",
        "                probs = []\n",
        "                for candidate in candidates:\n",
        "                    p = self.calculate_prob(ind_w, candidate, sent, 2)\n",
        "                    # print(candidate)\n",
        "                    # print(p)\n",
        "                    # print(calculate_prob(ind_w, candidate, sent, 5, fivegram_data, fivegram_total))\n",
        "                    probs.append(p)\n",
        "                if len(candidates) > 0:\n",
        "                    sent[ind_w] = candidates[np.argmax(probs)][0]\n",
        "                    was_changed = True\n",
        "        return sent\n",
        "\n"
      ],
      "metadata": {
        "id": "FvaFMCL78VEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_sent(sent, spell_funct):\n",
        "    \"\"\"\n",
        "    General function to correct sentnces\n",
        "    :param sent: sent with error\n",
        "    :param spell_funct: funct to spell check\n",
        "    :return: corrected sent\n",
        "    \"\"\"\n",
        "    new_sent = preprocess(sent)\n",
        "    renew_sent = [w.lower() for w in new_sent]\n",
        "    renew_sent = spell_funct(renew_sent.copy())\n",
        "    result = sent\n",
        "\n",
        "    for ind_w in range(len(new_sent)):\n",
        "        if renew_sent[ind_w].lower() != new_sent[ind_w].lower():\n",
        "            if new_sent[ind_w][0].isupper():\n",
        "                renew_sent[ind_w] = renew_sent[ind_w][0].upper() + renew_sent[ind_w][1:]\n",
        "                # renew_sent[ind_w][0].upper()\n",
        "\n",
        "            result = result.replace(new_sent[ind_w], renew_sent[ind_w])\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "9GfojzeF8mez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare needed class objects\n",
        "dictionary = Dictionary(bigram_path=\"bigrams.txt\", fivegram_path=\"fivegrams.txt\")\n",
        "edit_dist = EditDistance()\n",
        "my_spell_checker = ContextSpellChecker(dictionary, edit_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoZ1h7vcw9WY",
        "outputId": "acf3197c-a990-46db-cf8b-3a06594f5ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate Norvig's implementation\n",
            "Number of correct answers: 991 out of: 2000\n",
            "Accuracy: 0.4955\n",
            "Seconds for running 1 sentence: 0.009827772999999977\n",
            "\n",
            "Evaluate my implementation\n",
            "Number of correct answers: 1063 out of: 2000\n",
            "Accuracy: 0.5315\n",
            "Seconds for running 1 sentence: 0.062461154999999983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"You know that all Italia men loves pizza!\"\n",
        "print(correct_sent(sent, my_spell_checker.spell_check))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LluDtE7nvgPJ",
        "outputId": "bc06bd77-9c44-4843-bd1d-9dd836279f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You know that all Italian men loves pizza!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = \"Differences exikt between us.\"\n",
        "sent2 = \"An exikt strategy for bussiness is ...\"\n",
        "\n",
        "print(correct_sent(sent1, my_spell_checker.spell_check))\n",
        "print(correct_sent(sent2, my_spell_checker.spell_check))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVFlEQr5wTHM",
        "outputId": "7ca5ecb2-fc5c-4b0c-8c5f-bd42b8ca8b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences exist between us.\n",
            "An exit strategy for business is ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "Let's go trough my implementation, look on basic steps done and improvements applied. Note that the main ideas were taken\n",
        "\n",
        "### 1. ContextSpellChecker\n",
        "The main class (and algorithm workflow) is ContextSpellChecker. It takes a sequence of words and finds those who contains misspell (non-word errors based on list of unique words). For each such word it finds some candidates with word_trie, ranks them with edit distance and takes most probable one based on the context near it.\n",
        "\n",
        "### 2. Dictionary\n",
        "I created a class called Dictionary to handle data, such as bigrams and fivegrams data, list of unique words (used as dictionary words) and characters of a language. Note that I considered only English words, so characters of any other language will be considered as unknown character. Nevertheless, this implementation may be improved by adding other languages data (bigrams, unique words and characters).\n",
        "\n",
        "#### 2.1. Ngrams\n",
        "As a bigram and fivegram dataset [2], I used the data provided with this assignment. This data contains the most frequent n-grams from the one billion word Corpus of Contemporary American English (COCA).\n",
        "Note that the algorithm uses only bigrams, while the test set was generated with the use of fivegrams. COCA datasets are big enough to be used within this implementation. While threegrams or combination of 2-, 3- and 4-grams may improve the perfomance (for more details refer to [3]), the appropriate large enough and high quality data were not found.\n",
        "\n",
        "I analyzed the number of unique words in bigram and fivegram dataset (single words were combined in one list) and obtained that bigrams data contains much more unique words than fivegrams. That's why I parsed bigrams dataset to obtain a list of unique words.  \n",
        "\n",
        "#### 2.2. Speed improvements\n",
        "\n",
        "The problem I faced during running first version of my code is a slow processing of words. I remembered a trie structures that may improve search on text strings. One of that, CharTrie from pygtrie [4], I used to speed up bigram search. The key is a string of format (first_element + \" \" + second_element) and value is a probability of such a bigram. Note that this probability was obtained by dividing number of bigram occurence on the number of first word count in the dataset. According to lectures [1], the smoothing is a good improvement, so I used one with 1/num\\_unique\\_words as a base. Moreover, to consider context from both sides of the misspelled word, the candidate probability was calculated as a multiplication of conditional probabilities for a \"sliding window\" subsequence (for more details please refer to [3]).\n",
        "\n",
        "One more slow process was finding a similar word to the misspelled. The Edit distance algorithm is not so fast, and the number of unique words is big, so more optimized search was needed. As a solution, I made a vector from each word and put it inside the annoy algorithm [5]. A vector was made as a number of occurences of each english letter in the word and, additionally, unknown character (all symbols that are not English characters). The manhattan distance was used with annoy algorithm, as an approximation to edit distance. Then the edit distance was used with found words for precise ranking.\n",
        "\n",
        "### 3. EditDistance\n",
        "I used Damreu-Levenstein variant with QWERTY-based weights. That means that the weight for swap 2 letters is 1, while all letter pairs which share vertical border get 0.5 multiplier for replace and all letter pairs which share at least some horizontal border get 0.7 multiplier for replace. One could go further and elaborate to a more complex algorithm with the distance between letters on the keyboard [6]. I took QWERTY keyboard, because it is the most popular layout used worldwide, especially in English-speaking countries [7].\n",
        "\n",
        "Based on existing research [3, 8], it is reasonable to consider edit distances smaller than or equal to 2. So I considered one more improvement: if difference in lenghts of misspelled word and a candidate is greater than 2, this candidate do not analyzed further. The ranked candidates goes through ngrams' probability sort and top one is returned as the correction.\n",
        "\n",
        "### DatasetGenerator\n",
        "\n",
        "To test my and Norvig's implementations, I used 2 dataset. First one is a preprocessed misspells in GitHub commits [9], and the second one is synthesized by me. In github preprocessing, I skipped mistakes in insert/delete word, punctuation, lowercase and numbers, while others became a part of the github_data. To generate my dataset, I used fivegrams provided with the assignment. The first words are taken from a random row in the fivegrams dataset, and the next several words (till there is a combination of words) are taken with the probability of being after previous sequence. Then the error in a random word from a generated sequence is made. Note that for insert and delete opertion, the QWERTY edit distance is considered as a probability distribution of choise.\n",
        "\n",
        "According to results, the context sensitive spell checker made more accurate predictions, but takes a bit more time for real-life tasks. This happens because there may be a lot of unknown words with big amount of candidates and small edit distance, so a lot of computations for calculatins edit distance and searching for probabilities are made. Based on this, one may conclude that the results obtained are good for a context sensitive spell checker.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Used resources:\n",
        "\n",
        "\n",
        "[1] - Lectures and labs materials\n",
        "\n",
        "[2] - https://www.ngrams.info/download_coca.asp\n",
        "\n",
        "[3] - https://arxiv.org/pdf/1910.11242.pdf\n",
        "\n",
        "[4] - https://pygtrie.readthedocs.io/en/latest/#pygtrie.CharTrie\n",
        "\n",
        "[5] - https://github.com/spotify/annoy\n",
        "\n",
        "[6] - https://www.diva-portal.org/smash/get/diva2:1116701/FULLTEXT01.pdf\n",
        "\n",
        "[7] - https://strawpoll.com/most-popular-keyboard-layout\n",
        "\n",
        "[8] - https://www.irjet.net/archives/V8/i9/IRJET-V8I9316.pdf\n",
        "\n",
        "[9] - https://github.com/mhagiwara/github-typo-corpus/tree/master\n",
        "\n",
        "[10] - https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
        "\n",
        "[11] - https://norvig.com/spell-correct.html\n",
        "\n",
        "[12] - https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "[13] - https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
        "\n",
        "[14] - https://www.dcs.bbk.ac.uk/~roger/corpora.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W5u2Z0INMGSt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data for Norvigs implementation\n",
        "urls = [\"https://norvig.com/big.txt\", \"https://norvig.com/spell-testset1.txt\", \"https://norvig.com/spell-testset2.txt\"]\n",
        "filenames2 = [\"big.txt\", \"spell-testset1.txt\", \"spell-testset2.txt\"]\n",
        "for i in range(len(filenames2)):\n",
        "    urllib.request.urlretrieve(urls[i], filenames2[i])"
      ],
      "metadata": {
        "id": "A9vgJPNOw012"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Github dataset\n",
        "url = \"https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\"\n",
        "\n",
        "urllib.request.urlretrieve(url, \"github-typo-corpus.v1.0.0.jsonl.gz\")\n",
        "\n",
        "with gzip.open('github-typo-corpus.v1.0.0.jsonl.gz', 'rb') as f_in:\n",
        "    with open('github_corpus.jsonl', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "metadata": {
        "id": "M9A77pY9wosk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NorvigSpellChecker:\n",
        "    # Norvigs implementation\n",
        "    \"\"\"Spelling Corrector in Python 3; see http://norvig.com/spell-correct.html\n",
        "\n",
        "    Copyright (c) 2007-2016 Peter Norvig\n",
        "    MIT license: www.opensource.org/licenses/mit-license.php\n",
        "    \"\"\"\n",
        "\n",
        "    ################ Spelling Corrector\n",
        "\n",
        "    def __init__(self, words_filename='big.txt'):\n",
        "        self.WORDS = Counter(self.words(open(words_filename).read()))\n",
        "\n",
        "    def words(self, text):\n",
        "        return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def P(self, word):\n",
        "        N = sum(self.WORDS.values())\n",
        "        \"Probability of `word`.\"\n",
        "        return self.WORDS[word] / N\n",
        "\n",
        "    def correction(self, word):\n",
        "        \"Most probable spelling correction for word.\"\n",
        "        return max(self.candidates(word), key=self.P)\n",
        "\n",
        "    def candidates(self, word):\n",
        "        \"Generate possible spelling corrections for word.\"\n",
        "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
        "\n",
        "    def known(self, words):\n",
        "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "        return set(w for w in words if w in self.WORDS)\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"All edits that are one edit away from `word`.\"\n",
        "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word):\n",
        "        \"All edits that are two edits away from `word`.\"\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "\n",
        "    def spell_norvig(self, old_sent):\n",
        "        sent = old_sent.copy()\n",
        "\n",
        "        for ind_w in range(len(sent)):\n",
        "            sent[ind_w] = self.correction(sent[ind_w])\n",
        "        return sent\n"
      ],
      "metadata": {
        "id": "HwJJ8XdB8aoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetGenerator:\n",
        "    \"\"\"\n",
        "    To test my and Norvig's implementations, I used 2 dataset.\n",
        "    First one is a preprocessed misspells in GitHub commits[9], and the second one is synthesized by me.\n",
        "    In github preprocessing, I skipped mistakes in insert/delete word, punctuation, lowercase and numbers, while\n",
        "    others became a part of the github_data. To generate my dataset, I used fivegrams provided with the assignment.\n",
        "    The first words are taken from a random row in the fivegrams dataset, and the next several words (till there is a\n",
        "    combination of words) are taken with the probability of being after previous sequence. Then the error in a random\n",
        "    word from a generated sequence is made. Note that for insert and delete opertion, the QWERTY edit distance is\n",
        "    considered as a probability distribution of choise.\n",
        "    \"\"\"\n",
        "    def __init__(self, fivegram_data, replace_weight_funct):\n",
        "        self.fivegram_data = fivegram_data\n",
        "        self.replace_weight = replace_weight_funct\n",
        "        self.chars = \"A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z\".lower().split(', ')\n",
        "\n",
        "    def generate_sent_fivegram(self):\n",
        "        \"\"\"\n",
        "        Generate sentence based on fivegrams probabilities\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        n = np.random.randint(len(self.fivegram_data))\n",
        "        words = list(self.fivegram_data.iloc[n][['0', '1', '2']].values)\n",
        "        prob = 1\n",
        "        while prob != 0:\n",
        "            # Take probabilities\n",
        "            prob_words_data = self.fivegram_data[np.all(self.fivegram_data[['0', '1', '2']] == words[-3:], axis=1)]\n",
        "            if len(prob_words_data) == 0:\n",
        "                prob = 0\n",
        "            else:\n",
        "                # Select next word\n",
        "                words.append(np.random.choice(prob_words_data['3'].values,\n",
        "                                              p=prob_words_data['prob'].values / prob_words_data['prob'].values.sum()))\n",
        "\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def generate_error_word(self, word, n_possible_chars=3):\n",
        "        \"\"\"\n",
        "        Make an error in the word\n",
        "        :param word: initial word\n",
        "        :param n_possible_chars: how many possible to eplace chars generate (only 1 will be chosen)\n",
        "        :return: word with error\n",
        "        \"\"\"\n",
        "        ind = np.random.randint(0, high=len(word))\n",
        "        word = list(word)\n",
        "\n",
        "        error_types = ['ins', 'del', 'swap', 'repl']\n",
        "        error_type = np.random.choice(error_types)\n",
        "\n",
        "        if error_type == 'swap':\n",
        "            # Choose left or right swap\n",
        "            if len(word) == 1:\n",
        "                # Do not make\n",
        "                lr = 0\n",
        "            elif ind == 0:\n",
        "                lr = 1\n",
        "            elif ind == len(word) - 1:\n",
        "                lr = -1\n",
        "            else:\n",
        "                # Random left or right\n",
        "                lr = np.random.choice([-1, 1])\n",
        "\n",
        "            word[ind], word[ind + lr] = word[ind + lr], word[ind]\n",
        "        elif error_type == 'del':\n",
        "            del word[ind]\n",
        "        else:\n",
        "            # Choose a char to generate error\n",
        "            possible_chars = np.random.choice(self.chars, replace=False, size=n_possible_chars)\n",
        "            # Calculate probability of each char\n",
        "            # Based on qwerty distance\n",
        "            prob_chars = 1.1 - np.array([self.replace_weight(word[ind], c) for c in possible_chars])\n",
        "            prob_chars = prob_chars / prob_chars.sum()\n",
        "            # Choose an error char\n",
        "            error_char = np.random.choice(possible_chars, p=prob_chars)\n",
        "\n",
        "            if error_type == 'ins':\n",
        "                word.insert(ind + 1, error_char)\n",
        "            elif error_type == 'repl':\n",
        "                word[ind] = error_char\n",
        "\n",
        "        return \"\".join(word)\n",
        "\n",
        "    def generate_error_sent(self, sent, max_n_errors=2):\n",
        "        \"\"\"\n",
        "        Make errors in the sentence\n",
        "        :param sent: initial sent\n",
        "        :param max_n_errors: num of errors\n",
        "        :return: sent with misspells\n",
        "        \"\"\"\n",
        "        words = preprocess(sent)\n",
        "        n_errors = np.random.randint(1, high=min(max_n_errors, len(words) // 2) + 1)\n",
        "\n",
        "        # More words, more errors may be made\n",
        "        w_probs = np.array([len(w) for w in words])\n",
        "        w_probs = w_probs / w_probs.sum()\n",
        "        # replace=True meaning that multiple errors may be made in 1 word\n",
        "        for ind_w in np.random.choice(range(0, len(words)), replace=True, size=n_errors, p=w_probs):\n",
        "            words[ind_w] = self.generate_error_word(words[ind_w])\n",
        "\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def generate_dataset(self, data_len=1000):\n",
        "        \"\"\"\n",
        "        Generate dataset src-tgt.\n",
        "        To generate my dataset, I used fivegrams provided with the assignment. The first words are taken from a random\n",
        "        row in the fivegrams dataset, and the next several words (till there is a combination of words) are taken with\n",
        "        the probability of being after previous sequence. Then the error in a random word from a generated sequence is\n",
        "        made. Note that for insert and delete opertion, the QWERTY edit distance is considered as a probability\n",
        "        distribution of choise.\n",
        "        :param data_len: len of dataset\n",
        "        :return: pd df\n",
        "        \"\"\"\n",
        "        dataset = pd.DataFrame(columns=['src', 'tgt'])\n",
        "        for i in range(data_len):\n",
        "            sent = self.generate_sent_fivegram()\n",
        "            error_sent = self.generate_error_sent(sent)\n",
        "            dataset.loc[i] = [error_sent, sent]\n",
        "        return dataset\n",
        "\n",
        "    def get_github_dataset(self, data_path='github_corpus.jsonl', data_len=1000):\n",
        "        \"\"\"\n",
        "        I skipped mistakes in insert/delete word, punctuation, lowercase and numbers,\n",
        "        while others became a part of the github_data.\n",
        "        :param data_path:\n",
        "        :param data_len:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        github_data = pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
        "        n = 0\n",
        "        with jsonlines.open(data_path) as reader:\n",
        "            for obj in reader:\n",
        "                for edit in obj['edits']:\n",
        "                    # print(edit)\n",
        "                    flag = False\n",
        "                    for p in \"`'()/=:\":\n",
        "                        if p in edit['src']['text']:\n",
        "                            flag = True\n",
        "                    if not flag:\n",
        "                        src = [w.lower() for w in preprocess(edit['src']['text'])]\n",
        "                        tgt = [w.lower() for w in preprocess(edit['tgt']['text'])]\n",
        "                        if (len(src) == len(tgt)) and (src != tgt) and (len(src) >= 3):\n",
        "                            github_data.loc[len(github_data.index)] = [edit['src']['text'], edit['tgt']['text']]\n",
        "                            n += 1\n",
        "                if n >= data_len:\n",
        "                    break\n",
        "        return github_data\n"
      ],
      "metadata": {
        "id": "C7gJKys58c2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset(dataset, spell_funct):\n",
        "    \"\"\"\n",
        "    Test dataset on the data\n",
        "    :param dataset: pd df src-tgt\n",
        "    :param spell_funct: funct to spell check\n",
        "    :return: stats\n",
        "    \"\"\"\n",
        "    sum_time = 0\n",
        "    corrects = 0\n",
        "    n = 0\n",
        "    for ind, row in dataset.iterrows():\n",
        "        # Start timer\n",
        "        start_time = datetime.datetime.now()\n",
        "        # Get corrected sequence of sentences\n",
        "        result = spell_funct([w.lower() for w in preprocess(row['src'])])\n",
        "        dur = datetime.datetime.now() - start_time\n",
        "\n",
        "        # Calculate statistics\n",
        "        corrects += sum([result[i] == row['tgt'][i] for i in range(len(result))]) / len(result)\n",
        "        sum_time += dur.total_seconds()\n",
        "        n += 1\n",
        "    # Print result\n",
        "    print(\"Percentage of correct token predictions:\", corrects, \"out of:\", n)\n",
        "    print(\"Accuracy:\", round(corrects / n, 4))\n",
        "    print(\"Seconds for running 1 sentence:\", sum_time / n)\n",
        "\n",
        "    return corrects, sum_time, n\n"
      ],
      "metadata": {
        "id": "b9-CSGnI8dp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read fivegrams from a file and process\n",
        "fivegrams = dictionary.load_fivegrams(path=\"fivegrams.txt\")\n",
        "\n",
        "data_generator = DatasetGenerator(fivegrams, edit_dist.replace_weight)\n",
        "\n",
        "# Generate a dataset from scratch (based on fivegrams)\n",
        "# my_dataset = data_generator.generate_dataset(200)\n",
        "# Or load already generated one\n",
        "my_dataset = pd.read_csv(\"my_dataset.csv\")\n",
        "# Needs some preprocessing for faster algorithm run\n",
        "my_dataset = my_dataset.copy()\n",
        "my_dataset['tgt'] = my_dataset['tgt'].apply(str.lower).apply(preprocess)\n",
        "\n",
        "# Load and preprocess real world dataset\n",
        "github_data = data_generator.get_github_dataset('github_corpus.jsonl', data_len=3000)\n",
        "github_data = github_data.copy()\n",
        "github_data['tgt'] = github_data['tgt'].apply(str.lower).apply(preprocess)"
      ],
      "metadata": {
        "id": "i_hj7nt9x0QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with Norvig implementation\n",
        "norvig_spell_checker = NorvigSpellChecker()"
      ],
      "metadata": {
        "id": "3IOst1u0yV8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthesized dataset\n",
        "print(\"Evaluate Norvig's implementation\")\n",
        "norvig_res = test_dataset(my_dataset, norvig_spell_checker.spell_norvig)\n",
        "print()\n",
        "print(\"Evaluate my implementation\")\n",
        "my_res = test_dataset(my_dataset, my_spell_checker.spell_check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P9WT_NfzIDG",
        "outputId": "d45284ea-7de6-43fe-ce91-06d75d5bf8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate Norvig's implementation\n",
            "Percentage of correct token predictions: 1849.4152358490276 out of: 2000\n",
            "Accuracy: 0.9247\n",
            "Seconds for running 1 sentence: 0.013874333999999969\n",
            "\n",
            "Evaluate my implementation\n",
            "Percentage of correct token predictions: 1863.513500998629 out of: 2000\n",
            "Accuracy: 0.9318\n",
            "Seconds for running 1 sentence: 0.04509435900000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Github dataset\")\n",
        "print(\"Evaluate Norvig's implementation\")\n",
        "norvig_res = test_dataset(github_data, norvig_spell_checker.spell_norvig)\n",
        "print()\n",
        "print(\"Evaluate my implementation\")\n",
        "my_res = test_dataset(github_data, my_spell_checker.spell_check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPRwNIuIy9oN",
        "outputId": "a495a554-4fdb-42a0-ebdb-a4db11f27f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Github dataset\n",
            "Evaluate Norvig's implementation\n",
            "Percentage of correct token predictions: 2439.935522286997 out of: 3000\n",
            "Accuracy: 0.8133\n",
            "Seconds for running 1 sentence: 0.18702886599999988\n",
            "\n",
            "Evaluate my implementation\n",
            "Percentage of correct token predictions: 2606.556782879971 out of: 3000\n",
            "Accuracy: 0.8689\n",
            "Seconds for running 1 sentence: 0.15353176233333303\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}